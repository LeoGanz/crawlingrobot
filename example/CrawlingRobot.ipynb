{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCb7z8qthxj7",
    "outputId": "9989a973-536c-44c7-b5ed-1f808b3d20bf"
   },
   "outputs": [],
   "source": [
    "#!git clone git@github.com:micheltokic/crawlingrobot.git\n",
    "#!pip install -e crawlingrobot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "#os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import gym\n",
    "import gym_crawlingrobot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_robot_control (env):\n",
    "    \n",
    "    done = False\n",
    "    action = None\n",
    "    env.reset()\n",
    "    cum_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_UP or event.key == pygame.K_w:\n",
    "                    env.step(0)\n",
    "                    action = 0\n",
    "                elif event.key == pygame.K_RIGHT or event.key == pygame.K_d:\n",
    "                    action = 1\n",
    "                elif event.key == pygame.K_DOWN or event.key == pygame.K_s:\n",
    "                    action = 2\n",
    "                elif event.key == pygame.K_LEFT or event.key == pygame.K_a:\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_r:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "                if action:\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "                    action = None\n",
    "                    cum_reward += reward\n",
    "                    step += 1\n",
    "                    print (f\"step={step}, action={action}, reward={reward:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                    cum_reward = 0\n",
    "                    step = 0\n",
    "\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), render_intermediate_steps=True, plot_steps_per_episode=True)\n",
    "env.robot.mode = 2 # => Use WASD or Arrow Keys to control the robot's arms\n",
    "manual_robot_control (env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Q-Learning with discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0uw6R_AY2-zH",
    "outputId": "ddeb06e2-713e-45a4-9925-a5368bdce981"
   },
   "outputs": [],
   "source": [
    "# function maps the 2D observation (x, y) to a single state number n \n",
    "def obs_to_number(obs, obs_max):\n",
    "    return int(obs[0] * obs_max + obs[1])\n",
    "\n",
    "def q_agent(Q, obs_max, env, learn=True, render=False, alpha=1, gamma=0.95, epsilon=0.2, maxSteps=10000, episodes=200):\n",
    "    \n",
    "    print (f\"Q.shape={Q.shape}\")\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    for episode in range (episodes):\n",
    "        done = False\n",
    "        init_obs = env.reset()\n",
    "        state = obs_to_number(init_obs, obs_max)\n",
    "        step = 0\n",
    "        cum_reward =0 \n",
    "\n",
    "        while not done and step < maxSteps:\n",
    "\n",
    "            # action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            # perform action in environment\n",
    "            nextObs, reward, done, _ = env.step(action)\n",
    "            nextState = obs_to_number(nextObs, obs_max)\n",
    "            cum_reward += reward\n",
    "\n",
    "            # environment rendering\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "                # process pygame event loop\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                    elif event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_ESCAPE:\n",
    "                            pygame.quit()\n",
    "                            return\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "            # Q-learning\n",
    "            if learn:\n",
    "                Q[state, action] += alpha * (reward + gamma * np.max(Q[nextState]) - Q[state, action])\n",
    "\n",
    "            # time transition\n",
    "            state = nextState\n",
    "            step += 1\n",
    "            \n",
    "        res = 0\n",
    "        if len(env.robot.episode_time_results) > 0:\n",
    "            res = env.robot.episode_time_results[-1]\n",
    "        print(f\"episode={episode} took {step} steps => cumulative reward: {cum_reward:.2f}\")\n",
    "        \n",
    "    pygame.quit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Learn Q function (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700)\n",
    "\n",
    "# 2.1) Initialize Q function\n",
    "obs_max = env.observation_space.high[0] + 1  # currently 5\n",
    "Q = np.zeros([obs_max ** len(env.observation_space.high), env.action_space.n])\n",
    "q_filename = \"Qfunction.pkl\"\n",
    "\n",
    "# learn Q function\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, gamma=0.9, epsilon=0.1, episodes=100, render=False, learn=True)\n",
    "\n",
    "# write learned Q function to disc\n",
    "pickle.dump( Q, open( q_filename, \"wb\" ) )\n",
    "print (\"Wrote Q function to file: \", q_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Evaluate policy derived from Q function (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "# load Q function\n",
    "print (\"Loading Q function from file: \", q_filename)\n",
    "Q = pickle.load( open(q_filename, \"rb\" ) )\n",
    "\n",
    "# evalue Q function\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True)\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, episodes=20, epsilon=0.1, render=True, learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpxQT37sIuRK"
   },
   "source": [
    "# 3) PPO2 control with continuous actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines\n",
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rfj_Nq08JEJg",
    "outputId": "d0fbe109-ba32-45ab-b8e6-cb0ad51b3d7b"
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines import PPO2\n",
    "import gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback class for event loop cleanup\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "\n",
    "class PyGameEventLoopCallback(BaseCallback):\n",
    "    \n",
    "    render = False\n",
    "    \n",
    "    def __init__(self, verbose=0, render=False):\n",
    "        super(PyGameEventLoopCallback, self).__init__(verbose)\n",
    "        self.render = render\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        robot_env = self.training_env.venv.envs[0]\n",
    "        \n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    robot_env.robot.render_intermediate_steps = not robot_env.robot.render_intermediate_steps\n",
    "\n",
    "        if self.render:\n",
    "            robot_env.render()\n",
    "        \n",
    "        return bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "25QXzNWkJTEs",
    "outputId": "4385d29c-55aa-46e3-f6f8-966bfe82d6c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"\"\n",
    "ppo2 = \"ppo2\"\n",
    "os.makedirs(ppo2, exist_ok=True)\n",
    "\n",
    "def ppo2_learn(env, render=False):\n",
    "    env = VecNormalize(DummyVecEnv([lambda: Monitor(env, log_dir)]), norm_obs=True, norm_reward=True)\n",
    "    model = PPO2(MlpPolicy, env, verbose=1, learning_rate=0.15)\n",
    "    model.learn(total_timesteps=30000, callback=PyGameEventLoopCallback(render=render))\n",
    "    \n",
    "    #model.learn(total_timesteps=30000)\n",
    "    model.save(\"ppo2/ppo2_crawling_robot\")\n",
    "    env.save(\"ppo2/vec_normalize.pkl\")\n",
    " \n",
    "    del model, env\n",
    "\n",
    "\n",
    "def ppo2_run_policy(env, render=False, episodes=1):\n",
    "    env = DummyVecEnv([lambda: Monitor(env, log_dir)])\n",
    "    env = VecNormalize.load(\"ppo2/vec_normalize.pkl\", env)\n",
    "    model = PPO2.load(\"ppo2/ppo2_crawling_robot\")\n",
    "\n",
    "    # visualization callback\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "\n",
    "    env.training = False\n",
    "    \n",
    "    for e in range (episodes): \n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, _reward, done, info = env.step(action)\n",
    "            reward = env.get_original_reward() # returns the last unnormalized reward\n",
    "            cb._on_step()\n",
    "            cum_reward += reward[0]\n",
    "            step += 1\n",
    "            print (f\"episode={e}, step={step}, action={action}, reward={reward[0]:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train policy for 30000 timesteps (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "robot_env = gym.make('crawlingrobot-continuous-v1', goal_distance=2500)\n",
    "ppo2_learn(env=robot_env, render=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pygame.quit() # close any already opened simulation windows\n",
    "\n",
    "#robot_env_nogui = gym.make('crawlingrobot-continuous-v1', goal_distance=2500, plot_steps_per_episode=False, render_intermediate_steps=False)\n",
    "robot_env_gui = gym.make('crawlingrobot-continuous-v1', goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True, render_intermediate_steps=True)\n",
    "ppo2_run_policy(env=robot_env_gui, episodes=30, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CrawlingRobot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
