{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCb7z8qthxj7",
    "outputId": "9989a973-536c-44c7-b5ed-1f808b3d20bf"
   },
   "outputs": [],
   "source": [
    "#!git clone git@github.com:micheltokic/crawlingrobot.git\n",
    "#!pip install -e crawlingrobot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.7.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "#os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_robot_control (env):\n",
    "    \n",
    "    done = False\n",
    "    action = None\n",
    "    env.reset()\n",
    "    cum_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_UP or event.key == pygame.K_w:\n",
    "                    env.step(0)\n",
    "                    action = 0\n",
    "                elif event.key == pygame.K_RIGHT or event.key == pygame.K_d:\n",
    "                    action = 1\n",
    "                elif event.key == pygame.K_DOWN or event.key == pygame.K_s:\n",
    "                    action = 2\n",
    "                elif event.key == pygame.K_LEFT or event.key == pygame.K_a:\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_r:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "                if action:\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "                    action = None\n",
    "                    cum_reward += reward\n",
    "                    step += 1\n",
    "                    print (f\"step={step}, action={action}, reward={reward:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                    cum_reward = 0\n",
    "                    step = 0\n",
    "\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1, action=None, reward=0.99, cum_reward=0.99, done=False\n",
      "step=2, action=None, reward=0.01, cum_reward=1.00, done=False\n",
      "step=3, action=None, reward=7.24, cum_reward=8.24, done=False\n",
      "step=4, action=None, reward=13.38, cum_reward=21.63, done=False\n",
      "step=5, action=None, reward=22.67, cum_reward=44.29, done=False\n",
      "step=6, action=None, reward=23.59, cum_reward=67.89, done=False\n",
      "step=7, action=None, reward=18.93, cum_reward=86.81, done=False\n",
      "step=8, action=None, reward=6.89, cum_reward=93.70, done=False\n",
      "step=9, action=None, reward=-0.03, cum_reward=93.67, done=False\n",
      "step=10, action=None, reward=0.02, cum_reward=93.69, done=False\n",
      "step=11, action=None, reward=0.01, cum_reward=93.70, done=False\n",
      "step=12, action=None, reward=0.01, cum_reward=93.71, done=False\n",
      "step=13, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=14, action=None, reward=-0.02, cum_reward=93.69, done=False\n",
      "step=15, action=None, reward=0.01, cum_reward=93.70, done=False\n",
      "step=16, action=None, reward=-0.01, cum_reward=93.69, done=False\n",
      "step=17, action=None, reward=0.01, cum_reward=93.70, done=False\n",
      "step=18, action=None, reward=0.01, cum_reward=93.71, done=False\n",
      "step=19, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=20, action=None, reward=-0.00, cum_reward=93.71, done=False\n",
      "step=21, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=22, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=23, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=24, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=25, action=None, reward=0.00, cum_reward=93.71, done=False\n",
      "step=26, action=None, reward=-11.43, cum_reward=82.28, done=False\n",
      "step=27, action=None, reward=-19.82, cum_reward=62.46, done=False\n",
      "step=28, action=None, reward=-24.85, cum_reward=37.61, done=False\n",
      "step=29, action=None, reward=-21.96, cum_reward=15.65, done=False\n",
      "step=30, action=None, reward=0.00, cum_reward=15.65, done=False\n",
      "step=31, action=None, reward=0.00, cum_reward=15.65, done=False\n",
      "step=32, action=None, reward=0.00, cum_reward=15.65, done=False\n",
      "step=33, action=None, reward=0.00, cum_reward=15.65, done=False\n",
      "step=34, action=None, reward=0.00, cum_reward=15.65, done=False\n",
      "step=35, action=None, reward=0.00, cum_reward=15.65, done=False\n",
      "step=36, action=None, reward=0.02, cum_reward=15.66, done=False\n",
      "step=37, action=None, reward=0.01, cum_reward=15.67, done=False\n",
      "step=38, action=None, reward=7.24, cum_reward=22.92, done=False\n",
      "step=39, action=None, reward=13.39, cum_reward=36.30, done=False\n",
      "step=40, action=None, reward=0.00, cum_reward=36.30, done=False\n",
      "step=41, action=None, reward=22.67, cum_reward=58.97, done=False\n",
      "step=42, action=None, reward=24.25, cum_reward=83.22, done=False\n",
      "step=43, action=None, reward=18.79, cum_reward=102.01, done=False\n",
      "step=44, action=None, reward=6.20, cum_reward=108.21, done=False\n",
      "step=45, action=None, reward=0.00, cum_reward=108.21, done=False\n",
      "step=46, action=None, reward=0.00, cum_reward=108.21, done=False\n",
      "step=47, action=None, reward=-0.03, cum_reward=108.18, done=False\n",
      "step=48, action=None, reward=0.02, cum_reward=108.20, done=False\n",
      "step=49, action=None, reward=0.01, cum_reward=108.21, done=False\n",
      "step=50, action=None, reward=0.01, cum_reward=108.22, done=False\n",
      "step=51, action=None, reward=0.00, cum_reward=108.22, done=False\n",
      "step=52, action=None, reward=0.00, cum_reward=108.22, done=False\n",
      "step=53, action=None, reward=0.00, cum_reward=108.22, done=False\n",
      "step=54, action=None, reward=0.00, cum_reward=108.22, done=False\n",
      "step=55, action=None, reward=0.00, cum_reward=108.22, done=False\n",
      "step=56, action=None, reward=0.00, cum_reward=108.22, done=False\n",
      "step=57, action=None, reward=0.01, cum_reward=108.22, done=False\n",
      "step=58, action=None, reward=6.80, cum_reward=115.02, done=False\n",
      "step=59, action=None, reward=17.64, cum_reward=132.66, done=False\n",
      "step=60, action=None, reward=17.79, cum_reward=150.45, done=False\n",
      "step=61, action=None, reward=24.42, cum_reward=174.87, done=False\n",
      "step=62, action=None, reward=18.67, cum_reward=193.54, done=False\n",
      "step=63, action=None, reward=6.68, cum_reward=200.22, done=False\n",
      "step=64, action=None, reward=0.00, cum_reward=200.22, done=False\n",
      "step=65, action=None, reward=-0.00, cum_reward=200.22, done=False\n",
      "step=66, action=None, reward=0.01, cum_reward=200.23, done=False\n",
      "step=67, action=None, reward=0.01, cum_reward=200.23, done=False\n",
      "step=68, action=None, reward=0.00, cum_reward=200.24, done=False\n",
      "step=69, action=None, reward=0.00, cum_reward=200.24, done=False\n",
      "step=70, action=None, reward=0.00, cum_reward=200.24, done=False\n",
      "step=71, action=None, reward=0.00, cum_reward=200.24, done=False\n",
      "step=72, action=None, reward=0.00, cum_reward=200.24, done=False\n",
      "step=73, action=None, reward=0.00, cum_reward=200.24, done=False\n",
      "step=74, action=None, reward=7.22, cum_reward=207.45, done=False\n",
      "step=75, action=None, reward=13.36, cum_reward=220.82, done=False\n",
      "step=76, action=None, reward=0.00, cum_reward=220.82, done=False\n",
      "step=77, action=None, reward=0.00, cum_reward=220.82, done=False\n",
      "step=78, action=None, reward=0.00, cum_reward=220.82, done=False\n",
      "step=79, action=None, reward=22.66, cum_reward=243.47, done=False\n",
      "step=80, action=None, reward=24.61, cum_reward=268.08, done=False\n",
      "step=81, action=None, reward=19.03, cum_reward=287.11, done=False\n",
      "step=82, action=None, reward=-0.02, cum_reward=287.09, done=False\n",
      "step=83, action=None, reward=0.02, cum_reward=287.11, done=False\n",
      "step=84, action=None, reward=-0.00, cum_reward=287.11, done=False\n",
      "step=85, action=None, reward=0.00, cum_reward=287.11, done=False\n",
      "step=86, action=None, reward=0.00, cum_reward=287.11, done=False\n",
      "step=87, action=None, reward=0.00, cum_reward=287.11, done=False\n",
      "step=88, action=None, reward=0.01, cum_reward=287.12, done=False\n",
      "step=89, action=None, reward=7.23, cum_reward=294.35, done=False\n",
      "step=90, action=None, reward=16.64, cum_reward=310.99, done=False\n",
      "step=91, action=None, reward=0.00, cum_reward=310.99, done=False\n",
      "step=92, action=None, reward=0.00, cum_reward=310.99, done=False\n",
      "step=93, action=None, reward=20.05, cum_reward=331.03, done=False\n",
      "step=94, action=None, reward=23.15, cum_reward=354.18, done=False\n",
      "step=95, action=None, reward=18.84, cum_reward=373.02, done=False\n",
      "step=96, action=None, reward=7.35, cum_reward=380.38, done=False\n",
      "step=97, action=None, reward=-0.02, cum_reward=380.35, done=False\n",
      "step=98, action=None, reward=0.02, cum_reward=380.37, done=False\n",
      "step=99, action=None, reward=-0.01, cum_reward=380.36, done=False\n",
      "step=100, action=None, reward=0.01, cum_reward=380.38, done=False\n",
      "step=101, action=None, reward=0.00, cum_reward=380.38, done=False\n",
      "step=102, action=None, reward=0.00, cum_reward=380.38, done=False\n",
      "step=103, action=None, reward=0.00, cum_reward=380.38, done=False\n",
      "step=104, action=None, reward=0.00, cum_reward=380.38, done=False\n",
      "step=105, action=None, reward=0.01, cum_reward=380.39, done=False\n",
      "step=106, action=None, reward=0.00, cum_reward=380.39, done=False\n",
      "step=107, action=None, reward=6.81, cum_reward=387.20, done=False\n",
      "step=108, action=None, reward=13.70, cum_reward=400.90, done=False\n",
      "step=109, action=None, reward=0.00, cum_reward=400.90, done=False\n",
      "step=110, action=None, reward=0.00, cum_reward=400.90, done=False\n",
      "step=111, action=None, reward=0.00, cum_reward=400.90, done=False\n",
      "step=112, action=None, reward=0.00, cum_reward=400.90, done=False\n",
      "step=113, action=None, reward=0.00, cum_reward=400.90, done=False\n",
      "step=114, action=None, reward=22.73, cum_reward=423.63, done=False\n",
      "step=115, action=None, reward=23.22, cum_reward=446.85, done=False\n",
      "step=116, action=None, reward=18.10, cum_reward=464.95, done=False\n",
      "step=117, action=None, reward=7.81, cum_reward=472.76, done=False\n",
      "step=118, action=None, reward=0.00, cum_reward=472.76, done=False\n",
      "step=119, action=None, reward=0.00, cum_reward=472.76, done=False\n",
      "step=120, action=None, reward=-0.02, cum_reward=472.73, done=False\n",
      "step=121, action=None, reward=0.02, cum_reward=472.75, done=False\n",
      "step=122, action=None, reward=-0.01, cum_reward=472.75, done=False\n",
      "step=123, action=None, reward=0.01, cum_reward=472.76, done=False\n",
      "step=124, action=None, reward=0.00, cum_reward=472.76, done=False\n",
      "step=125, action=None, reward=0.00, cum_reward=472.76, done=False\n",
      "step=126, action=None, reward=0.00, cum_reward=472.76, done=False\n",
      "step=127, action=None, reward=0.00, cum_reward=472.76, done=False\n",
      "step=128, action=None, reward=0.00, cum_reward=472.76, done=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=129, action=None, reward=0.01, cum_reward=472.77, done=False\n",
      "step=130, action=None, reward=0.00, cum_reward=472.77, done=False\n",
      "step=131, action=None, reward=6.81, cum_reward=479.58, done=False\n",
      "step=132, action=None, reward=14.24, cum_reward=493.83, done=False\n",
      "step=133, action=None, reward=0.00, cum_reward=493.83, done=False\n",
      "step=134, action=None, reward=0.00, cum_reward=493.83, done=False\n",
      "step=135, action=None, reward=0.00, cum_reward=493.83, done=False\n",
      "step=136, action=None, reward=22.75, cum_reward=516.58, done=False\n",
      "step=137, action=None, reward=23.18, cum_reward=539.76, done=False\n",
      "step=138, action=None, reward=17.48, cum_reward=557.24, done=True\n",
      "step=1, action=None, reward=0.97, cum_reward=0.97, done=False\n",
      "step=2, action=None, reward=0.01, cum_reward=0.98, done=False\n",
      "step=3, action=None, reward=-0.01, cum_reward=0.97, done=False\n",
      "step=4, action=None, reward=0.01, cum_reward=0.97, done=False\n",
      "step=5, action=None, reward=-0.02, cum_reward=0.95, done=False\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), render_intermediate_steps=True, plot_steps_per_episode=True)\n",
    "env.robot.mode = 2 # => Use WASD or Arrow Keys to control the robot's arms\n",
    "manual_robot_control (env)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Q-Learning with discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0uw6R_AY2-zH",
    "outputId": "ddeb06e2-713e-45a4-9925-a5368bdce981"
   },
   "outputs": [],
   "source": [
    "# function maps the 2D observation (x, y) to a single state number n \n",
    "def obs_to_number(obs, obs_max):\n",
    "    return int(obs[0] * obs_max + obs[1])\n",
    "\n",
    "def q_agent(Q, obs_max, env, learn=True, render=False, alpha=1, gamma=0.95, epsilon=0.2, maxSteps=10000, episodes=200):\n",
    "    \n",
    "    print (f\"Q.shape={Q.shape}\")\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    for episode in range (episodes):\n",
    "        done = False\n",
    "        init_obs = env.reset()\n",
    "        state = obs_to_number(init_obs, obs_max)\n",
    "        step = 0\n",
    "        cum_reward =0 \n",
    "\n",
    "        while not done and step < maxSteps:\n",
    "            \n",
    "                # process pygame event loop\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                    elif event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_ESCAPE:\n",
    "                            pygame.quit()\n",
    "                            return\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "                # action selection\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(Q[state])\n",
    "\n",
    "                # perform action in environment\n",
    "                nextObs, reward, done, _ = env.step(action)\n",
    "                cum_reward += reward\n",
    "                \n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                nextState = obs_to_number(nextObs, obs_max)\n",
    "\n",
    "                # Q-learning\n",
    "                if learn:\n",
    "                    Q[state, action] += alpha * (reward + gamma * np.max(Q[nextState]) - Q[state, action])\n",
    "\n",
    "                # time transition\n",
    "                state = nextState\n",
    "                step += 1\n",
    "        res = 0\n",
    "        if len(env.robot.episode_time_results) > 0:\n",
    "            res = env.robot.episode_time_results[-1]\n",
    "        print(f\"episode={episode} took {step} steps => cumulative reward: {cum_reward:.2f}\")\n",
    "        \n",
    "    pygame.quit()\n",
    "    return\n",
    "\n",
    "#########################\n",
    "# Initialize Q function\n",
    "#########################\n",
    "obs_max = env.observation_space.high[0] + 1  # currently 5\n",
    "Q = np.zeros([obs_max ** len(env.observation_space.high), env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Learn Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=(25, 4)\n",
      "episode=0 took 1028 steps => cumulative reward: 517.27\n",
      "episode=1 took 156 steps => cumulative reward: 512.80\n",
      "episode=2 took 167 steps => cumulative reward: 510.91\n",
      "episode=3 took 226 steps => cumulative reward: 510.32\n",
      "episode=4 took 149 steps => cumulative reward: 510.94\n",
      "episode=5 took 163 steps => cumulative reward: 515.71\n",
      "episode=6 took 121 steps => cumulative reward: 510.71\n",
      "episode=7 took 117 steps => cumulative reward: 522.51\n",
      "episode=8 took 118 steps => cumulative reward: 514.04\n",
      "episode=9 took 106 steps => cumulative reward: 523.96\n",
      "episode=10 took 108 steps => cumulative reward: 522.98\n",
      "episode=11 took 98 steps => cumulative reward: 512.74\n",
      "episode=12 took 116 steps => cumulative reward: 513.77\n",
      "episode=13 took 111 steps => cumulative reward: 519.10\n",
      "episode=14 took 123 steps => cumulative reward: 527.61\n",
      "episode=15 took 105 steps => cumulative reward: 520.53\n",
      "episode=16 took 99 steps => cumulative reward: 514.06\n",
      "episode=17 took 90 steps => cumulative reward: 511.06\n",
      "episode=18 took 127 steps => cumulative reward: 526.84\n",
      "episode=19 took 107 steps => cumulative reward: 517.29\n",
      "episode=20 took 111 steps => cumulative reward: 516.09\n",
      "episode=21 took 80 steps => cumulative reward: 513.41\n",
      "episode=22 took 84 steps => cumulative reward: 522.70\n",
      "episode=23 took 91 steps => cumulative reward: 519.97\n",
      "episode=24 took 94 steps => cumulative reward: 523.49\n",
      "episode=25 took 82 steps => cumulative reward: 518.90\n",
      "episode=26 took 89 steps => cumulative reward: 526.12\n",
      "episode=27 took 84 steps => cumulative reward: 523.73\n",
      "episode=28 took 90 steps => cumulative reward: 516.38\n",
      "episode=29 took 79 steps => cumulative reward: 523.37\n",
      "episode=30 took 88 steps => cumulative reward: 515.58\n",
      "episode=31 took 89 steps => cumulative reward: 510.37\n",
      "episode=32 took 88 steps => cumulative reward: 519.28\n",
      "episode=33 took 73 steps => cumulative reward: 514.48\n",
      "episode=34 took 125 steps => cumulative reward: 528.13\n",
      "episode=35 took 75 steps => cumulative reward: 516.44\n",
      "episode=36 took 82 steps => cumulative reward: 529.64\n",
      "episode=37 took 83 steps => cumulative reward: 521.78\n",
      "episode=38 took 88 steps => cumulative reward: 512.10\n",
      "episode=39 took 99 steps => cumulative reward: 520.98\n",
      "episode=40 took 82 steps => cumulative reward: 512.62\n",
      "episode=41 took 76 steps => cumulative reward: 529.31\n",
      "episode=42 took 89 steps => cumulative reward: 514.15\n",
      "episode=43 took 89 steps => cumulative reward: 518.86\n",
      "episode=44 took 83 steps => cumulative reward: 528.97\n",
      "episode=45 took 92 steps => cumulative reward: 512.66\n",
      "episode=46 took 116 steps => cumulative reward: 518.09\n",
      "episode=47 took 90 steps => cumulative reward: 522.47\n",
      "episode=48 took 82 steps => cumulative reward: 525.31\n",
      "episode=49 took 95 steps => cumulative reward: 511.83\n",
      "episode=50 took 93 steps => cumulative reward: 520.33\n",
      "episode=51 took 100 steps => cumulative reward: 518.02\n",
      "episode=52 took 79 steps => cumulative reward: 514.91\n",
      "episode=53 took 86 steps => cumulative reward: 517.25\n",
      "episode=54 took 88 steps => cumulative reward: 513.95\n",
      "episode=55 took 98 steps => cumulative reward: 515.32\n",
      "episode=56 took 96 steps => cumulative reward: 525.15\n",
      "episode=57 took 82 steps => cumulative reward: 517.76\n",
      "episode=58 took 101 steps => cumulative reward: 510.56\n",
      "episode=59 took 80 steps => cumulative reward: 525.30\n",
      "episode=60 took 83 steps => cumulative reward: 526.85\n",
      "episode=61 took 94 steps => cumulative reward: 518.67\n",
      "episode=62 took 83 steps => cumulative reward: 529.53\n",
      "episode=63 took 80 steps => cumulative reward: 519.69\n",
      "episode=64 took 78 steps => cumulative reward: 521.62\n",
      "episode=65 took 88 steps => cumulative reward: 511.09\n",
      "episode=66 took 102 steps => cumulative reward: 517.80\n",
      "episode=67 took 86 steps => cumulative reward: 514.17\n",
      "episode=68 took 99 steps => cumulative reward: 532.12\n",
      "episode=69 took 96 steps => cumulative reward: 514.36\n",
      "episode=70 took 88 steps => cumulative reward: 515.62\n",
      "episode=71 took 82 steps => cumulative reward: 510.18\n",
      "episode=72 took 84 steps => cumulative reward: 519.55\n",
      "episode=73 took 109 steps => cumulative reward: 524.38\n",
      "episode=74 took 101 steps => cumulative reward: 511.20\n",
      "episode=75 took 88 steps => cumulative reward: 529.45\n",
      "episode=76 took 83 steps => cumulative reward: 521.61\n",
      "episode=77 took 82 steps => cumulative reward: 527.09\n",
      "episode=78 took 87 steps => cumulative reward: 511.09\n",
      "episode=79 took 77 steps => cumulative reward: 511.51\n",
      "episode=80 took 82 steps => cumulative reward: 527.52\n",
      "episode=81 took 87 steps => cumulative reward: 520.02\n",
      "episode=82 took 90 steps => cumulative reward: 515.13\n",
      "episode=83 took 84 steps => cumulative reward: 516.57\n",
      "episode=84 took 85 steps => cumulative reward: 525.78\n",
      "episode=85 took 88 steps => cumulative reward: 510.01\n",
      "episode=86 took 76 steps => cumulative reward: 510.29\n",
      "episode=87 took 71 steps => cumulative reward: 523.99\n",
      "episode=88 took 83 steps => cumulative reward: 511.50\n",
      "episode=89 took 91 steps => cumulative reward: 515.01\n",
      "episode=90 took 95 steps => cumulative reward: 521.08\n",
      "episode=91 took 92 steps => cumulative reward: 513.17\n",
      "episode=92 took 83 steps => cumulative reward: 511.23\n",
      "episode=93 took 88 steps => cumulative reward: 519.11\n",
      "episode=94 took 79 steps => cumulative reward: 518.17\n",
      "episode=95 took 99 steps => cumulative reward: 514.18\n",
      "episode=96 took 90 steps => cumulative reward: 515.29\n",
      "episode=97 took 84 steps => cumulative reward: 515.15\n",
      "episode=98 took 85 steps => cumulative reward: 515.02\n",
      "episode=99 took 83 steps => cumulative reward: 525.56\n"
     ]
    }
   ],
   "source": [
    "pygame.quit()\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700)\n",
    "\n",
    "# learn Q function\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, gamma=0.9, epsilon=0.1, episodes=100, render=False, learn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Evaluate policy derived from Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=(25, 4)\n",
      "episode=0 took 75 steps => cumulative reward: 527.24\n",
      "episode=1 took 75 steps => cumulative reward: 527.24\n",
      "episode=2 took 75 steps => cumulative reward: 527.24\n"
     ]
    }
   ],
   "source": [
    "# evalue Q function\n",
    "pygame.quit()\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True)\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, episodes=20, epsilon=0, render=True, learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpxQT37sIuRK"
   },
   "source": [
    "# 3) PPO2 control with continuous actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines\n",
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rfj_Nq08JEJg",
    "outputId": "d0fbe109-ba32-45ab-b8e6-cb0ad51b3d7b"
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines import PPO2\n",
    "import gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback class for event loop cleanup\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "\n",
    "class PyGameEventLoopCallback(BaseCallback):\n",
    "    \n",
    "    render = False\n",
    "    \n",
    "    def __init__(self, verbose=0, render=False):\n",
    "        super(PyGameEventLoopCallback, self).__init__(verbose)\n",
    "        self.render = render\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        robot_env = self.training_env.venv.envs[0]\n",
    "        \n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    robot_env.robot.render_intermediate_steps = not robot_env.robot.render_intermediate_steps\n",
    "\n",
    "        if self.render:\n",
    "            robot_env.render()\n",
    "        \n",
    "        return bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "25QXzNWkJTEs",
    "outputId": "4385d29c-55aa-46e3-f6f8-966bfe82d6c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"\"\n",
    "ppo2 = \"ppo2\"\n",
    "os.makedirs(ppo2, exist_ok=True)\n",
    "\n",
    "def ppo2_learn(env, render=False):\n",
    "    env = VecNormalize(DummyVecEnv([lambda: Monitor(env, log_dir)]), norm_obs=True, norm_reward=True)\n",
    "    model = PPO2(MlpPolicy, env, verbose=1, learning_rate=0.15)\n",
    "    model.learn(total_timesteps=30000, callback=PyGameEventLoopCallback(render=render))\n",
    "    \n",
    "    #model.learn(total_timesteps=30000)\n",
    "    model.save(\"ppo2/ppo2_crawling_robot\")\n",
    "    env.save(\"ppo2/vec_normalize.pkl\")\n",
    " \n",
    "    del model, env\n",
    "\n",
    "\n",
    "def ppo2_run_policy(env, render=False, episodes=1):\n",
    "    env = DummyVecEnv([lambda: Monitor(env, log_dir)])\n",
    "    env = VecNormalize.load(\"ppo2/vec_normalize.pkl\", env)\n",
    "    model = PPO2.load(\"ppo2/ppo2_crawling_robot\")\n",
    "\n",
    "    # visualization callback\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "\n",
    "    env.training = False\n",
    "    \n",
    "    for e in range (episodes): \n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, _reward, done, info = env.step(action)\n",
    "            reward = env.get_original_reward() # returns the last unnormalized reward\n",
    "            cb._on_step()\n",
    "            cum_reward += reward[0]\n",
    "            step += 1\n",
    "            print (f\"episode={e}, step={step}, action={action}, reward={reward[0]:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train policy for 30000 timesteps (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robot_env = gym.make('crawlingrobot-continuous-v1', goal_distance=2500)\n",
    "ppo2_learn(env=robot_env, render=False)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#robot_env_nogui = gym.make('crawlingrobot-continuous-v1', goal_distance=2500, plot_steps_per_episode=False, render_intermediate_steps=False)\n",
    "robot_env_gui = gym.make('crawlingrobot-continuous-v1', goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True, render_intermediate_steps=True)\n",
    "ppo2_run_policy(env=robot_env_gui, episodes=30, render=True)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CrawlingRobot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
