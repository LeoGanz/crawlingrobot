{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCb7z8qthxj7",
    "outputId": "9989a973-536c-44c7-b5ed-1f808b3d20bf"
   },
   "outputs": [],
   "source": [
    "#!git clone git@github.com:micheltokic/crawlingrobot.git\n",
    "#!pip install -e crawlingrobot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "#os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_robot_control (env):\n",
    "    \n",
    "    done = False\n",
    "    action = None\n",
    "    env.reset()\n",
    "    cum_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                elif event.key == pygame.K_UP or event.key == pygame.K_w:\n",
    "                    env.step(0)\n",
    "                    action = 0\n",
    "                elif event.key == pygame.K_RIGHT or event.key == pygame.K_d:\n",
    "                    action = 1\n",
    "                elif event.key == pygame.K_DOWN or event.key == pygame.K_s:\n",
    "                    action = 2\n",
    "                elif event.key == pygame.K_LEFT or event.key == pygame.K_a:\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_r:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                elif event.key == pygame.K_SPACE:\n",
    "                    env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "                if action:\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "                    action = None\n",
    "                    cum_reward += reward\n",
    "                    step += 1\n",
    "                    print (f\"step={step}, action={action}, reward={reward:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n",
    "                if done:\n",
    "                    env.reset()\n",
    "                    action = 3\n",
    "                    cum_reward = 0\n",
    "                    step = 0\n",
    "\n",
    "            env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), render_intermediate_steps=True, plot_steps_per_episode=True)\n",
    "env.robot.mode = 2 # => Use WASD or Arrow Keys to control the robot's arms\n",
    "manual_robot_control (env)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Q-Learning with discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0uw6R_AY2-zH",
    "outputId": "ddeb06e2-713e-45a4-9925-a5368bdce981"
   },
   "outputs": [],
   "source": [
    "# function maps the 2D observation (x, y) to a single state number n \n",
    "def obs_to_number(obs, obs_max):\n",
    "    return int(obs[0] * obs_max + obs[1])\n",
    "\n",
    "def q_agent(Q, obs_max, env, learn=True, render=False, alpha=1, gamma=0.95, epsilon=0.2, maxSteps=10000, episodes=200):\n",
    "    \n",
    "    print (f\"Q.shape={Q.shape}\")\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    for episode in range (episodes):\n",
    "        done = False\n",
    "        init_obs = env.reset()\n",
    "        state = obs_to_number(init_obs, obs_max)\n",
    "        step = 0\n",
    "        cum_reward =0 \n",
    "\n",
    "        while not done and step < maxSteps:\n",
    "\n",
    "            # action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            # perform action in environment\n",
    "            nextObs, reward, done, _ = env.step(action)\n",
    "            nextState = obs_to_number(nextObs, obs_max)\n",
    "            cum_reward += reward\n",
    "\n",
    "            # environment rendering\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "                # process pygame event loop\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        return\n",
    "                    elif event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_ESCAPE:\n",
    "                            pygame.quit()\n",
    "                            return\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            env.robot.render_intermediate_steps = not env.robot.render_intermediate_steps\n",
    "\n",
    "            # Q-learning\n",
    "            if learn:\n",
    "                Q[state, action] += alpha * (reward + gamma * np.max(Q[nextState]) - Q[state, action])\n",
    "\n",
    "            # time transition\n",
    "            state = nextState\n",
    "            step += 1\n",
    "            \n",
    "        res = 0\n",
    "        if len(env.robot.episode_time_results) > 0:\n",
    "            res = env.robot.episode_time_results[-1]\n",
    "        print(f\"episode={episode} took {step} steps => cumulative reward: {cum_reward:.2f}\")\n",
    "        \n",
    "    pygame.quit()\n",
    "    return\n",
    "\n",
    "#########################\n",
    "# Initialize Q function\n",
    "#########################\n",
    "obs_max = env.observation_space.high[0] + 1  # currently 5\n",
    "Q = np.zeros([obs_max ** len(env.observation_space.high), env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Learn Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=(25, 4)\n",
      "episode=0 took 642 steps => cumulative reward: 510.92\n",
      "episode=1 took 102 steps => cumulative reward: 526.93\n",
      "episode=2 took 106 steps => cumulative reward: 521.42\n",
      "episode=3 took 97 steps => cumulative reward: 524.49\n",
      "episode=4 took 99 steps => cumulative reward: 524.60\n",
      "episode=5 took 91 steps => cumulative reward: 517.62\n",
      "episode=6 took 106 steps => cumulative reward: 526.31\n",
      "episode=7 took 96 steps => cumulative reward: 516.85\n",
      "episode=8 took 102 steps => cumulative reward: 510.40\n",
      "episode=9 took 101 steps => cumulative reward: 520.16\n",
      "episode=10 took 98 steps => cumulative reward: 510.63\n",
      "episode=11 took 90 steps => cumulative reward: 529.07\n",
      "episode=12 took 95 steps => cumulative reward: 519.99\n",
      "episode=13 took 93 steps => cumulative reward: 529.75\n",
      "episode=14 took 99 steps => cumulative reward: 525.11\n",
      "episode=15 took 90 steps => cumulative reward: 527.26\n",
      "episode=16 took 86 steps => cumulative reward: 526.94\n",
      "episode=17 took 84 steps => cumulative reward: 527.95\n",
      "episode=18 took 82 steps => cumulative reward: 515.07\n",
      "episode=19 took 87 steps => cumulative reward: 512.17\n",
      "episode=20 took 87 steps => cumulative reward: 527.28\n",
      "episode=21 took 96 steps => cumulative reward: 510.98\n",
      "episode=22 took 82 steps => cumulative reward: 510.81\n",
      "episode=23 took 79 steps => cumulative reward: 518.11\n",
      "episode=24 took 85 steps => cumulative reward: 517.84\n",
      "episode=25 took 84 steps => cumulative reward: 512.30\n",
      "episode=26 took 81 steps => cumulative reward: 511.93\n",
      "episode=27 took 86 steps => cumulative reward: 513.55\n",
      "episode=28 took 75 steps => cumulative reward: 510.30\n",
      "episode=29 took 74 steps => cumulative reward: 523.60\n",
      "episode=30 took 87 steps => cumulative reward: 520.74\n",
      "episode=31 took 70 steps => cumulative reward: 524.46\n",
      "episode=32 took 79 steps => cumulative reward: 531.34\n",
      "episode=33 took 76 steps => cumulative reward: 523.60\n",
      "episode=34 took 79 steps => cumulative reward: 514.90\n",
      "episode=35 took 78 steps => cumulative reward: 530.64\n",
      "episode=36 took 73 steps => cumulative reward: 514.40\n",
      "episode=37 took 73 steps => cumulative reward: 517.40\n",
      "episode=38 took 78 steps => cumulative reward: 528.32\n",
      "episode=39 took 74 steps => cumulative reward: 511.91\n",
      "episode=40 took 73 steps => cumulative reward: 520.84\n",
      "episode=41 took 81 steps => cumulative reward: 519.49\n",
      "episode=42 took 79 steps => cumulative reward: 526.30\n",
      "episode=43 took 74 steps => cumulative reward: 522.38\n",
      "episode=44 took 79 steps => cumulative reward: 524.25\n",
      "episode=45 took 81 steps => cumulative reward: 515.63\n",
      "episode=46 took 79 steps => cumulative reward: 529.95\n",
      "episode=47 took 75 steps => cumulative reward: 522.67\n",
      "episode=48 took 68 steps => cumulative reward: 521.59\n",
      "episode=49 took 71 steps => cumulative reward: 520.55\n",
      "episode=50 took 73 steps => cumulative reward: 519.90\n",
      "episode=51 took 72 steps => cumulative reward: 523.86\n",
      "episode=52 took 72 steps => cumulative reward: 516.27\n",
      "episode=53 took 83 steps => cumulative reward: 514.17\n",
      "episode=54 took 79 steps => cumulative reward: 526.09\n",
      "episode=55 took 79 steps => cumulative reward: 517.82\n",
      "episode=56 took 86 steps => cumulative reward: 522.30\n",
      "episode=57 took 88 steps => cumulative reward: 522.86\n",
      "episode=58 took 94 steps => cumulative reward: 520.22\n",
      "episode=59 took 77 steps => cumulative reward: 519.80\n",
      "episode=60 took 75 steps => cumulative reward: 522.94\n",
      "episode=61 took 69 steps => cumulative reward: 529.35\n",
      "episode=62 took 83 steps => cumulative reward: 519.25\n",
      "episode=63 took 99 steps => cumulative reward: 517.60\n",
      "episode=64 took 74 steps => cumulative reward: 526.89\n",
      "episode=65 took 73 steps => cumulative reward: 532.01\n",
      "episode=66 took 76 steps => cumulative reward: 529.13\n",
      "episode=67 took 79 steps => cumulative reward: 516.93\n",
      "episode=68 took 75 steps => cumulative reward: 523.72\n",
      "episode=69 took 69 steps => cumulative reward: 532.58\n",
      "episode=70 took 70 steps => cumulative reward: 531.30\n",
      "episode=71 took 71 steps => cumulative reward: 517.32\n",
      "episode=72 took 65 steps => cumulative reward: 512.67\n",
      "episode=73 took 69 steps => cumulative reward: 532.58\n",
      "episode=74 took 75 steps => cumulative reward: 526.38\n",
      "episode=75 took 76 steps => cumulative reward: 526.90\n",
      "episode=76 took 92 steps => cumulative reward: 528.31\n",
      "episode=77 took 80 steps => cumulative reward: 532.72\n",
      "episode=78 took 76 steps => cumulative reward: 515.01\n",
      "episode=79 took 74 steps => cumulative reward: 516.68\n",
      "episode=80 took 67 steps => cumulative reward: 510.97\n",
      "episode=81 took 80 steps => cumulative reward: 520.61\n",
      "episode=82 took 76 steps => cumulative reward: 519.08\n",
      "episode=83 took 77 steps => cumulative reward: 516.97\n",
      "episode=84 took 68 steps => cumulative reward: 520.14\n",
      "episode=85 took 75 steps => cumulative reward: 511.69\n",
      "episode=86 took 71 steps => cumulative reward: 521.82\n",
      "episode=87 took 74 steps => cumulative reward: 519.61\n",
      "episode=88 took 78 steps => cumulative reward: 511.01\n",
      "episode=89 took 88 steps => cumulative reward: 515.89\n",
      "episode=90 took 71 steps => cumulative reward: 515.85\n",
      "episode=91 took 73 steps => cumulative reward: 512.94\n",
      "episode=92 took 79 steps => cumulative reward: 515.75\n",
      "episode=93 took 89 steps => cumulative reward: 523.49\n",
      "episode=94 took 89 steps => cumulative reward: 530.65\n",
      "episode=95 took 82 steps => cumulative reward: 514.01\n",
      "episode=96 took 82 steps => cumulative reward: 510.51\n",
      "episode=97 took 71 steps => cumulative reward: 533.26\n",
      "episode=98 took 71 steps => cumulative reward: 516.57\n",
      "episode=99 took 76 steps => cumulative reward: 525.04\n"
     ]
    }
   ],
   "source": [
    "pygame.quit()\n",
    "\n",
    "# instantiate environment\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700)\n",
    "\n",
    "# learn Q function\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, gamma=0.9, epsilon=0.1, episodes=100, render=False, learn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Evaluate policy derived from Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=(25, 4)\n",
      "episode=0 took 83 steps => cumulative reward: 525.37\n",
      "episode=1 took 76 steps => cumulative reward: 518.50\n",
      "episode=2 took 73 steps => cumulative reward: 510.60\n",
      "episode=3 took 90 steps => cumulative reward: 514.26\n",
      "episode=4 took 75 steps => cumulative reward: 513.55\n",
      "episode=5 took 72 steps => cumulative reward: 525.98\n",
      "episode=6 took 79 steps => cumulative reward: 519.48\n",
      "episode=7 took 82 steps => cumulative reward: 521.05\n",
      "episode=8 took 99 steps => cumulative reward: 516.18\n",
      "episode=9 took 83 steps => cumulative reward: 527.85\n",
      "episode=10 took 76 steps => cumulative reward: 513.33\n",
      "episode=11 took 78 steps => cumulative reward: 513.62\n",
      "episode=12 took 82 steps => cumulative reward: 526.64\n",
      "episode=13 took 74 steps => cumulative reward: 513.35\n",
      "episode=14 took 79 steps => cumulative reward: 519.14\n",
      "episode=15 took 75 steps => cumulative reward: 510.83\n",
      "episode=16 took 86 steps => cumulative reward: 516.32\n",
      "episode=17 took 79 steps => cumulative reward: 521.67\n",
      "episode=18 took 80 steps => cumulative reward: 514.84\n",
      "episode=19 took 93 steps => cumulative reward: 515.58\n"
     ]
    }
   ],
   "source": [
    "# evalue Q function\n",
    "pygame.quit()\n",
    "env = gym.make('crawlingrobot-discrete-v1', rotation_angles=5, goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True)\n",
    "q_agent(Q=Q, obs_max=obs_max, env=env, episodes=20, epsilon=0.1, render=True, learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpxQT37sIuRK"
   },
   "source": [
    "# 3) PPO2 control with continuous actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines\n",
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rfj_Nq08JEJg",
    "outputId": "d0fbe109-ba32-45ab-b8e6-cb0ad51b3d7b"
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines import PPO2\n",
    "import gym\n",
    "import gym_crawlingrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback class for event loop cleanup\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "\n",
    "class PyGameEventLoopCallback(BaseCallback):\n",
    "    \n",
    "    render = False\n",
    "    \n",
    "    def __init__(self, verbose=0, render=False):\n",
    "        super(PyGameEventLoopCallback, self).__init__(verbose)\n",
    "        self.render = render\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        robot_env = self.training_env.venv.envs[0]\n",
    "        \n",
    "        # process pygame event loop\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    robot_env.robot.render_intermediate_steps = not robot_env.robot.render_intermediate_steps\n",
    "\n",
    "        if self.render:\n",
    "            robot_env.render()\n",
    "        \n",
    "        return bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "25QXzNWkJTEs",
    "outputId": "4385d29c-55aa-46e3-f6f8-966bfe82d6c4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"\"\n",
    "ppo2 = \"ppo2\"\n",
    "os.makedirs(ppo2, exist_ok=True)\n",
    "\n",
    "def ppo2_learn(env, render=False):\n",
    "    env = VecNormalize(DummyVecEnv([lambda: Monitor(env, log_dir)]), norm_obs=True, norm_reward=True)\n",
    "    model = PPO2(MlpPolicy, env, verbose=1, learning_rate=0.15)\n",
    "    model.learn(total_timesteps=30000, callback=PyGameEventLoopCallback(render=render))\n",
    "    \n",
    "    #model.learn(total_timesteps=30000)\n",
    "    model.save(\"ppo2/ppo2_crawling_robot\")\n",
    "    env.save(\"ppo2/vec_normalize.pkl\")\n",
    " \n",
    "    del model, env\n",
    "\n",
    "\n",
    "def ppo2_run_policy(env, render=False, episodes=1):\n",
    "    env = DummyVecEnv([lambda: Monitor(env, log_dir)])\n",
    "    env = VecNormalize.load(\"ppo2/vec_normalize.pkl\", env)\n",
    "    model = PPO2.load(\"ppo2/ppo2_crawling_robot\")\n",
    "\n",
    "    # visualization callback\n",
    "    cb = PyGameEventLoopCallback(render=render)\n",
    "    cb.training_env = env\n",
    "\n",
    "    env.training = False\n",
    "    \n",
    "    for e in range (episodes): \n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, _reward, done, info = env.step(action)\n",
    "            reward = env.get_original_reward() # returns the last unnormalized reward\n",
    "            cb._on_step()\n",
    "            cum_reward += reward[0]\n",
    "            step += 1\n",
    "            print (f\"episode={e}, step={step}, action={action}, reward={reward[0]:.2f}, cum_reward={cum_reward:.2f}, done={done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train policy for 30000 timesteps (no GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robot_env = gym.make('crawlingrobot-continuous-v1', goal_distance=2500)\n",
    "ppo2_learn(env=robot_env, render=False)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate policy (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#robot_env_nogui = gym.make('crawlingrobot-continuous-v1', goal_distance=2500, plot_steps_per_episode=False, render_intermediate_steps=False)\n",
    "robot_env_gui = gym.make('crawlingrobot-continuous-v1', goal_distance=700, window_size=(640, 480), plot_steps_per_episode=True, render_intermediate_steps=True)\n",
    "ppo2_run_policy(env=robot_env_gui, episodes=30, render=True)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CrawlingRobot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
